{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c07bb0-cf78-46af-b80a-0af021695f3c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5911c93-700f-4c08-bb8f-d8a62bc6776e",
   "metadata": {},
   "source": [
    "This project is initiated by [Raphael Jeong-Hin Chin](https://jeonghin.com/) to create a Deep Learning (DL) model that can be implemented in automated kiosks to help people with difficulties ordering goods and services through the kiosks. This project was primarily focused on the Penang Chinese community, and thus, the DL model is trained with the Penang Hokkien Dialect for this purpose. Nonetheless, similar techniques can be applied to other languages and dialects with modifications on the model. \n",
    "\n",
    "The primary motivation for this project was due to this [article](https://guangming.com.my/kfc%E4%BD%BF%E7%94%A8%E9%9B%BB%E5%AD%90%E8%87%AA%E5%8A%A9%E9%BB%9E%E9%A4%90-%E7%B6%B2%E7%B4%85%E6%84%9F%E6%85%A8%E9%9B%A3%E7%82%BA%E8%80%81%E4%BA%BA%E5%AE%B6) \\[1\\] that talks about the difficulties and challenges faced by elderly people while ordering KFC through its automated kiosk. The lack of human interaction in the process of purchasing goods and services has caused difficulties to elderly customers and various research was conducted to solve this issue. This project is created to serve as a proof-of-concept for introducing DL models into automated kiosks so that customers can order goods and services through the Hokkien Dialect. \n",
    "\n",
    "Penang Hokkien was chosen because it is a subdivision of Hokkien which the author is fluent in. Hokkien is the most spoken dialect by the Chinese ethnic group not only in Penang, but also throughout Malaysia and Singapore. For example, the word kiasu is used to describe the grasping or selfish attitude arising from a fear of missing out on something. This Hokkien word has been included in the Oxford English Dictionary as a legitimate Singaporean English word. Moreover, the Penang Hokkien is almost similar to other types of Hokkien spoken in Malaysia, Singapore, and Taiwan. Therefore, choosing Hokkien as a dialect to train on can help solve the challenges posed by the automated kiosks and preserve the Hokkien dialect.\n",
    "\n",
    "The voice samples were collected from eight different individuals. Each individual will say each digit (1 to 10) in Hokkien once. The samples will not published with this notebook. Individuals who are interested in the samples should contact the author at jeonghin@gmail.com. A majority portion of the codes contained in this notebook came from Ketan Doshi. \\[2\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832f810-7f04-49b9-92dd-41377d02b89c",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972b2256-f606-4847-b7ed-2331c6404df9",
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1653261907985,
     "user": {
      "displayName": "Raphael Jeong-Hin Chin",
      "userId": "00873680198078168651"
     },
     "user_tz": 240
    },
    "id": "972b2256-f606-4847-b7ed-2331c6404df9"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import torch\n",
    "wav_names = []\n",
    "wav_labels = []\n",
    "tot_samples = 8 # Total number of samples\n",
    "for sample_size in range(1,tot_samples+1):\n",
    "    for true_class in range(1,11):\n",
    "        wav_names.append(\"/sample\"+str(sample_size)+\"_\"+str(true_class)+\".wav\")\n",
    "        wav_labels.append(str(true_class))\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(wav_labels)\n",
    "# targets: array([0, 1, 2, 3])\n",
    "\n",
    "targets = torch.as_tensor(targets)\n",
    "# targets: tensor([0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4fad542-064f-428c-83d2-d7b51268ed21",
   "metadata": {
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1653261910916,
     "user": {
      "displayName": "Raphael Jeong-Hin Chin",
      "userId": "00873680198078168651"
     },
     "user_tz": 240
    },
    "id": "b4fad542-064f-428c-83d2-d7b51268ed21"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'relative_path':wav_names, 'classID':wav_labels})\n",
    "data_path = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31cb2249-093a-43e0-a7e3-bbc7304539c1",
   "metadata": {
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1653261912397,
     "user": {
      "displayName": "Raphael Jeong-Hin Chin",
      "userId": "00873680198078168651"
     },
     "user_tz": 240
    },
    "id": "31cb2249-093a-43e0-a7e3-bbc7304539c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "\n",
    "class AudioUtil():\n",
    "    # ----------------------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert the given audio to the desired number of channels\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sig.shape[0] == new_channel):\n",
    "            # Nothing to do\n",
    "            return aud\n",
    "\n",
    "        if (new_channel == 1):\n",
    "            # Convert from stereo to mono by selecting only the first channel\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            # Convert from mono to stereo by duplicating the first channel\n",
    "            resig = torch.cat([sig, sig])\n",
    "\n",
    "        return ((resig, sr))\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sr == newsr):\n",
    "            # Nothing to do\n",
    "            return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "        # Resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "            # Resample the second channel and merge both channels\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return ((resig, newsr))\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "\n",
    "        if (sig_len > max_len):\n",
    "            # Truncate the signal to the given length\n",
    "            sig = sig[:,:max_len]\n",
    "\n",
    "        elif (sig_len < max_len):\n",
    "            # Length of padding to add at the beginning and end of the signal\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "            # Pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "\n",
    "        return (sig, sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Shifts the signal to the left or right by some percent. Values at the end\n",
    "    # are 'wrapped around' to the start of the transformed signal.\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig,sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Generate a Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig,sr = aud\n",
    "        top_db = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "        # Convert to decibels\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "    # overfitting and to help the model generalise better. The masked sections are\n",
    "    # replaced with the mean value.\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfb2f8d-f1df-4191-85d0-685b20694fb0",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1653261912527,
     "user": {
      "displayName": "Raphael Jeong-Hin Chin",
      "userId": "00873680198078168651"
     },
     "user_tz": 240
    },
    "id": "3cfb2f8d-f1df-4191-85d0-685b20694fb0"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "\n",
    "    # ----------------------------\n",
    "    # Number of items in dataset\n",
    "    # ----------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "\n",
    "    # ----------------------------\n",
    "    # Get i'th item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n",
    "        # Get the Class ID\n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "        # majority. So make all sounds have the same number of channels and same \n",
    "        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "        # result in arrays of different lengths, even though the sound duration is\n",
    "        # the same.\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "        return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d648af-51eb-4962-90b6-60ec8d6c8231",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1653261912927,
     "user": {
      "displayName": "Raphael Jeong-Hin Chin",
      "userId": "00873680198078168651"
     },
     "user_tz": 240
    },
    "id": "89d648af-51eb-4962-90b6-60ec8d6c8231"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "myds = SoundDS(df, data_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=num_val, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=num_val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984c4e47-e952-4c50-bab2-526898e55eae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1653261913597,
     "user": {
      "displayName": "Raphael Jeong-Hin Chin",
      "userId": "00873680198078168651"
     },
     "user_tz": 240
    },
    "id": "984c4e47-e952-4c50-bab2-526898e55eae",
    "outputId": "dcad3c76-60ca-4de1-b9ce-2a6093e5f060"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "        conv_layers += [self.conv5, self.relu5, self.bn5]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=256, out_features=10)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70351e00-9a5b-4497-809b-748aa34a1582",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5372,
     "status": "ok",
     "timestamp": 1653262225060,
     "user": {
      "displayName": "Raphael Jeong-Hin Chin",
      "userId": "00873680198078168651"
     },
     "user_tz": 240
    },
    "id": "70351e00-9a5b-4497-809b-748aa34a1582",
    "outputId": "38676eab-1148-467a-8b1f-df4118fa9c7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.33, Accuracy: 0.09\n",
      "Epoch: 1, Loss: 2.33, Accuracy: 0.20\n",
      "Epoch: 2, Loss: 2.41, Accuracy: 0.14\n",
      "Epoch: 3, Loss: 2.32, Accuracy: 0.11\n",
      "Epoch: 4, Loss: 2.20, Accuracy: 0.17\n",
      "Epoch: 5, Loss: 2.33, Accuracy: 0.17\n",
      "Epoch: 6, Loss: 2.17, Accuracy: 0.20\n",
      "Epoch: 7, Loss: 2.18, Accuracy: 0.12\n",
      "Epoch: 8, Loss: 2.05, Accuracy: 0.23\n",
      "Epoch: 9, Loss: 2.14, Accuracy: 0.20\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            # inputs, labels =  data[0].to(device), torch.as_tensor(data[1]).to(device)\n",
    "            # inputs, labels = data[0], data[1]\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            targets = le.fit_transform(list(data[1]))\n",
    "            inputs, labels = data[0], torch.as_tensor(targets)\n",
    "            # print(targets)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "            #if i % 10 == 0:    # print every 10 mini-batches\n",
    "            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction/total_prediction\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "num_epochs= 10   # Just for demo, adjust this higher.\n",
    "training(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4f710c5-1a16-42db-bc74-37ff9b278f0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 787,
     "status": "ok",
     "timestamp": 1653262440540,
     "user": {
      "displayName": "Raphael Jeong-Hin Chin",
      "userId": "00873680198078168651"
     },
     "user_tz": 240
    },
    "id": "e4f710c5-1a16-42db-bc74-37ff9b278f0d",
    "outputId": "f4e63e4c-7f14-4d35-f911-b8a078b6bbec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.12, Total items: 16\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, val_dl):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "        for data in val_dl:\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            # inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # inputs, labels = data[0], data[1]\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            targets = le.fit_transform(list(data[1]))\n",
    "            inputs, labels = data[0], torch.as_tensor(targets)\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "      \n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "inference(myModel, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61f58a-cb8f-4df2-b330-8926a61476eb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The result is not satisfying as the train and test accuracies are too low. Small amount of data is one of the causes for the low accuracy as the model did not have enough data to train on. One way to solve it is to collect more samples. \n",
    "\n",
    "As this is just a personal project and no funding was provided, it is difficult to reach out to the public to collect voice samples. However, if this personal project makes sense and research groups out there are interested to make it into a real research project, feel free to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb7b33-6899-4da8-b79f-34962a7babda",
   "metadata": {
    "id": "ospmi0_8G55T"
   },
   "source": [
    "## References\n",
    "\n",
    "\\[1\\] https://guangming.com.my/kfc%E4%BD%BF%E7%94%A8%E9%9B%BB%E5%AD%90%E8%87%AA%E5%8A%A9%E9%BB%9E%E9%A4%90-%E7%B6%B2%E7%B4%85%E6%84%9F%E6%85%A8%E9%9B%A3%E7%82%BA%E8%80%81%E4%BA%BA%E5%AE%B6\n",
    "\n",
    "\\[2\\] https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Hokkien_DL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
